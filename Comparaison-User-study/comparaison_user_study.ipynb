{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`comparaison_env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os \n",
    "import sys\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import entropy, pearsonr\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "sys.path.append(os.path.abspath(\"/home/pelissier/These-ATER/Papier_international3/Dataset\"))\n",
    "from utils import *\n",
    "sys.path.append('/home/pelissier/These-ATER/Papier_international3/Code')  # Adjust the path based on the relative location\n",
    "from utils_comparaison import *\n",
    "from metriques import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODELNET40 REMESHING ISO\n",
    "ModelNet40_aligned_us = \"/home/pelissier/These-ATER/Papier_international3/Code/Comparaison-User-study/Alignement/Dataset-aligned\"\n",
    "# Data des 12 caméras du Rendu\n",
    "data_modelnet_cam = pd.read_csv(\"/home/pelissier/These-ATER/Papier_international3/Dataset/Rendu/ModelNet40/circular_config_12_elevation_30_R22.csv\")\n",
    "# Path ok (avec limper + projection + bvs)\n",
    "paths_bvs = read_paths_from_txt(\"/home/pelissier/These-ATER/Papier_international3/Dataset/paths_files/obj_SMPLER_files_ModelNet40_remeshing_iso_user-study-844-ok.txt\"); print(\"Fichiers bvs de Modelnet40 :\", len(paths_bvs))\n",
    "dir_bvs = \"/home/pelissier/These-ATER/Papier_international3/Dataset/Rendu/ModelNet40/bvs_remeshing_iso\"\n",
    "dict_bvs_dispo = {cat : int(np.sum([1 for i in range(len(paths_bvs)) if cat in paths_bvs[i]])) for cat in set(paths_bvs[i].split('/')[0] for i in range(len(paths_bvs)))}\n",
    "# BVS dispo en foncton des axe de sym\n",
    "dict_bvs_dispo_axe_sym = {}\n",
    "for cat_m in set(paths_bvs[i].split('/')[0] for i in range(len(paths_bvs))):\n",
    "    paths_bvs_cat = [paths_bvs[i] for i in range(len(paths_bvs)) if cat_m in paths_bvs[i]]\n",
    "    paths_and_axes_sym = read_paths_from_txt(\"/home/pelissier/These-ATER/Papier_international3/Code/Comparaison-User-study/Alignement/paths/\"+cat_m+\"_meshes_and_axe.txt\")\n",
    "    dict_bvs_dispo_axe_sym[cat_m] = {'0':[], '1':[], '2':[], '3': [], '4': [], '+': []}\n",
    "    for path in paths_and_axes_sym:\n",
    "        num_path = path.split('_SMPLER')[0].split(cat_m+\"_\")[-1]\n",
    "        # si on a le fichier bvs \n",
    "        if int(np.sum([num_path in paths_bvs_cat[u] for u in range(len(paths_bvs_cat))])):\n",
    "            path_obj = path.split(',')[0].replace('Dataset-aligned/', '').replace(\"_aligned_ok_US.obj\",\"\")\n",
    "            if ',1' in path: dict_bvs_dispo_axe_sym[cat_m]['1'].append(path_obj)\n",
    "            elif ',2' in path: dict_bvs_dispo_axe_sym[cat_m]['2'].append(path_obj)\n",
    "            elif ',3' in path: dict_bvs_dispo_axe_sym[cat_m]['3'].append(path_obj)\n",
    "            elif ',4' in path: dict_bvs_dispo_axe_sym[cat_m]['4'].append(path_obj)\n",
    "            elif ',0' in path: dict_bvs_dispo_axe_sym[cat_m]['0'].append(path_obj)\n",
    "            else: dict_bvs_dispo_axe_sym[cat_m]['+'].append(path_obj)\n",
    "##################################################################################################################################\n",
    "### User study\n",
    "# Path to the directory containing the csv files of the user study\n",
    "dir_us = \"/home/pelissier/These-ATER/Papier_internationale2/Validation/user_study/3D/post_traitement/csv_etude_Prolific\"\n",
    "# Data of camera poses in user study : i, j, theta, delta, X, Y, Z\n",
    "data_us_cam = pd.read_csv(\"/home/pelissier/These-ATER/Papier_international3/Code/Comparaison-User-study/cam_pose_rep_etude_arrondi.csv\")\n",
    "label_us_cam = pd.read_csv(\"/home/pelissier/These-ATER/Papier_international3/Code/Comparaison-User-study/cam_rep_etude_label_arrondi.csv\")\n",
    "data_us_cam['label'] = list(label_us_cam['label'])\n",
    "## Choix et bvs des 44 modeles \n",
    "dir_bvs_us = '/home/pelissier/These-ATER/Papier_internationale2/Validation/user_study/3D/post_traitement/csv_etude_Prolific/csv_etude_filtre/visualisation_filtre'\n",
    "paths_bvs_us_csv = glob.glob(os.path.join(dir_bvs_us, \"**\", \"*global_distribution_label.csv*\"), recursive=True); print(\"Modeles de US : \",len(paths_bvs_us_csv))\n",
    "# Path data folder of user study\n",
    "dir_Data = \"/home/pelissier/These-ATER/Papier_internationale2/Data\"\n",
    "\n",
    "##################################################################################################################################\n",
    "# Correspondances entre les noms des modèles dans ModelNet10 et les noms des modèles dans l'User Study\n",
    "match_ModelNet2US = {'airplane': 'A380', \"chair\":'chair107', 'bench': 'bench', 'dresser': 'cabinet-d', 'night_stand': 'cabinet-n', 'tv_stand': 'cabinet-t', 'vase': 'vase', 'cup':'cup', 'car': 'carVasa'}\n",
    "# Outputs tmp\n",
    "# Path of user-study outputs folder in Dataset\n",
    "dir_outputs_visu = \"/home/pelissier/These-ATER/Papier_international3/Code/Comparaison-User-study/visualisation_cams/\"\n",
    "dir_outputs_csv = \"/home/pelissier/These-ATER/Papier_international3/Code/Comparaison-User-study/csv_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_categorie_us = ['carVasa', 'cup', \"A380\", \"cabinet-d\", \"cabinet-t\", \"bench\", \"chair107\", \"cabinet-n\", \"vase\"]\n",
    "list_categroie_us_sym = ['carVasa', 'cup', \"A380\", \"cabinet-d\", \"cabinet-t\", \"bench\", \"cabinet-n\", \"vase\"]\n",
    "\n",
    "## Symetrie considérée\n",
    "## Tous les objets peut importe le nb d'axe de symétrie\n",
    "dossier_courant = \"All/\"\n",
    "dir_outputs_visu = os.path.join(dir_outputs_visu, dossier_courant); \n",
    "dir_outputs_csv = os.path.join(dir_outputs_csv, dossier_courant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contraintes : quelles caméras de l'US on considère ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Condition\n",
    "## Toutes les caméras \n",
    "#j_cam_us_ok = [0,1,2,3,4]\n",
    "\n",
    "## Que les cameras de la couronne j==1\n",
    "j_cam_us_ok = [1]; dir_outputs_visu = os.path.join(dir_outputs_visu, \"8cams/\"); dir_outputs_csv = os.path.join(dir_outputs_csv, \"8cams/\")\n",
    "\n",
    "## Que les cameras de la couronne j==1 ou j == 2\n",
    "#j_cam_us_ok = [1,2]; dir_outputs_visu = os.path.join(dir_outputs_visu, \"16cams/\"); dir_outputs_csv = os.path.join(dir_outputs_csv, \"16cams/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User study\n",
    "# Coordonnées des caméras de l'étude utilateur\n",
    "X_us = []; Y_us = []; Z_us = []; labels_us = []; I_us = []; J_us = []\n",
    "for j in j_cam_us_ok:\n",
    "    X_us += list(data_us_cam.loc[data_us_cam['j'] == j]['X_rep_etude'])\n",
    "    Y_us += list(data_us_cam.loc[data_us_cam['j'] == j]['Y_rep_etude'])\n",
    "    Z_us += list(data_us_cam.loc[data_us_cam['j'] == j]['Z_rep_etude'])\n",
    "    I_us += list(data_us_cam.loc[data_us_cam['j'] == j]['i'])\n",
    "    J_us += [j for _ in range(8)]\n",
    "    labels_us += list(data_us_cam.loc[data_us_cam['j'] == j]['label']) \n",
    "cams_us = np.around(np.column_stack((X_us, Y_us, Z_us, np.array([1]*len(X_us)))),3)\n",
    "R_sphere = list(data_us_cam['R'])[0]\n",
    "\n",
    "## ModeleNet \n",
    "# Coordonnées des 12 caméras de ModelNet40 dans le repère de ModelNet40\n",
    "X_modelnet = np.array(data_modelnet_cam['LocationX'][1:])\n",
    "Y_modelnet = np.array(data_modelnet_cam['LocationY'][1:])\n",
    "Z_modelnet = np.array(data_modelnet_cam['LocationZ'][1:])\n",
    "cams_modelnet = np.column_stack((X_modelnet, Y_modelnet, Z_modelnet, np.array([1]*12)))\n",
    "print(\"12 cams Modelnet dans repère Modelnet : \\n\", cams_modelnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cams_us), \"cams US\", len(labels_us), \"lables US\")\n",
    "print(str(len(cams_us))+\" cams US considérées dans repère US : \\n\", cams_us, labels_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poids + BVS user study --> en fonction des caméras de l'us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bvs_us = {}\n",
    "# Les POV qui nous intéressent sont ceux sur la couronne 'Milieu-Dessus', donc avec j == 1\n",
    "for path_bvs_us_csv in paths_bvs_us_csv:\n",
    "    name = os.path.basename(path_bvs_us_csv).split('_')[0]\n",
    "    df = pd.read_csv(path_bvs_us_csv)\n",
    "    # on ne garde que les info de caméra concidérées\n",
    "    filter_df = df[df['label'].isin(labels_us)]\n",
    "    filter_sorted_df = filter_df.sort_values(by='poids', ascending=False)\n",
    "    # BVS : attention il put y avoir plusieurs BVS pour un même modèle <=> plusieurs labels avec le même poids\n",
    "    label_bvs_mesh = list(filter_sorted_df.loc[filter_sorted_df['poids'] ==  max(list(filter_sorted_df['poids']))]['label'])\n",
    "    position_bvs_mesh = [[i for i in range(len(labels_us)) if labels_us[i] in label_bvs_mesh]]\n",
    "    # coordonnées 3D des caméras BVS dans le repère de l'US\n",
    "    cam_bvs = np.concatenate([np.array(X_us)[position_bvs_mesh], np.array(Y_us)[position_bvs_mesh], np.array(Z_us)[position_bvs_mesh]]).T  \n",
    "    # Indice I-J des caméras dans le repère de l'US\n",
    "    i_j_cam = np.concatenate([np.array(I_us)[position_bvs_mesh], np.array(J_us)[position_bvs_mesh]]).T\n",
    "    all_bvs_us[name] = {'df': filter_sorted_df, 'label_bvs': label_bvs_mesh, 'cam_bvs': cam_bvs, 'ij_bvs': i_j_cam, 'poids_bvs' :  max(list(filter_sorted_df['poids']))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relancer partie suivante quand tous les bvs seront calculés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour 1 categorie : cvs avec poids par caméras US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_ModelNet2US.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorie_modelnet = \"vase\"\n",
    "categorie_us = match_ModelNet2US[categorie_modelnet]; print(categorie_modelnet, categorie_us)\n",
    "if 'cabinet' in categorie_us: categorie_us = 'cabinet'\n",
    "print(categorie_modelnet, categorie_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modele de l'US asscoié à la catégorie\n",
    "path_mesh_us = os.path.join(dir_Data, categorie_us, categorie_us+\"_update_normed_centered_user_study.obj\")\n",
    "#path_mesh_us = os.path.join(dir_Data, 'mesh_objectnet_regulier', categorie_us, categorie_us+\"_regulier_tri_normed_centered_user_study.obj\")\n",
    "print(os.path.exists(path_mesh_us))\n",
    "\n",
    "# dossier pour les visualisations\n",
    "if not os.path.exists(os.path.join(dir_outputs_visu, categorie_modelnet)):\n",
    "    os.makedirs(os.path.join(dir_outputs_visu, categorie_modelnet), exist_ok=True)\n",
    "dir_outputs_visu_categorie = os.path.join(dir_outputs_visu, categorie_modelnet); print(dir_outputs_visu_categorie)\n",
    "if not os.path.exists(os.path.join(dir_outputs_csv, categorie_modelnet)):\n",
    "    os.makedirs(os.path.join(dir_outputs_csv, categorie_modelnet), exist_ok=True)\n",
    "dir_outputs_csv_categorie = os.path.join(dir_outputs_csv, categorie_modelnet); print(dir_outputs_csv_categorie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact/Poids de la BVS de modelnet aligné sur les X cams US --> BVS sur l'ensemble des modèles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fichiers BVS de la categorie\n",
    "paths_bvs_cat = [paths_bvs[i] for i in range(len(paths_bvs)) if categorie_modelnet in paths_bvs[i]]; print(\"Fichiers bvs de la categorie dispo :\", len(paths_bvs_cat), paths_bvs_cat[0])\n",
    "random.shuffle(paths_bvs_cat)\n",
    "print(paths_bvs_cat[0])\n",
    "\n",
    "## Fichiers BVS dispo en fonction des axes de symetrie\n",
    "# path des obj + nb axe sym  \n",
    "paths_and_axes_sym = read_paths_from_txt(\"/home/pelissier/These-ATER/Papier_international3/Code/Comparaison-User-study/Alignement/paths/\"+categorie_modelnet+\"_meshes_and_axe.txt\"); print(\"\\n\",len(paths_and_axes_sym))\n",
    "dict_axes_sym = {'0':[], '1':[], '2':[], '3': [], '4': [], '+': []}\n",
    "for path in paths_and_axes_sym:\n",
    "    num_path = path.split('_SMPLER')[0].split(categorie_modelnet+\"_\")[-1]\n",
    "    # si on a le fichier bvs \n",
    "    if int(np.sum([num_path in paths_bvs_cat[u] for u in range(len(paths_bvs_cat))])):\n",
    "        path_obj = path.split(',')[0].replace('Dataset-aligned/', '').replace(\"_aligned_ok_US.obj\",\"\")\n",
    "        if ',1' in path: dict_axes_sym['1'].append(path_obj)\n",
    "        elif ',2' in path: dict_axes_sym['2'].append(path_obj)\n",
    "        elif ',3' in path: dict_axes_sym['3'].append(path_obj)\n",
    "        elif ',4' in path: dict_axes_sym['4'].append(path_obj)\n",
    "        elif ',0' in path: dict_axes_sym['0'].append(path_obj)\n",
    "        else: dict_axes_sym['+'].append(path_obj)\n",
    "    \n",
    "print(dict_axes_sym['0'][:2], dict_axes_sym['2'][:2])\n",
    "## Verification \n",
    "total = 0\n",
    "for key in dict_axes_sym:\n",
    "    total += len(dict_axes_sym[key])\n",
    "print(f\"Total number of models: {total}\")   \n",
    "if total != len(paths_bvs_cat): print(\"Attention, il manque des fichiers bvs pour la catégorie\", categorie_modelnet) \n",
    "axes_sym = ['All']+[k for k in dict_axes_sym.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création des fichiers CAT_distribution_global_modelnet-Xcams.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ## En fonction des axes de symétrie\n",
    "    for axe_sym in axes_sym[:0]:\n",
    "        print(axe_sym)\n",
    "        if axe_sym == 'All' : paths_bvs_cat_axe = paths_bvs_cat; axe = \"\"\n",
    "        else : paths_bvs_cat_axe = dict_axes_sym[axe_sym]; axe = str(axe_sym)+\"_axe_sym_\"\n",
    "        \n",
    "        ## Df des poids de chaque mesh de la catégorie sur les caméras de l'US\n",
    "        df_poids_from_modelnet = pd.DataFrame(columns=['path', 'name']+[f\"{int(I_us[k])}-{J_us[k]}\" for k in range(len(I_us))])\n",
    "        df_poids_from_modelnet.loc[0] = [None, 'labels US'] + labels_us\n",
    "            \n",
    "        # Pour chacun des modèles de la catégorie étudée dont on a le fichier bvs (que 38 actuellement), issue de ModelNet40 (car : 296)\n",
    "        for n in tqdm.tqdm(range(len(paths_bvs_cat_axe))):\n",
    "            ###################################################################\n",
    "            ## ModelNet40\n",
    "            # Load model alignés à l'étude utilisateur\n",
    "            path_mesh_n = paths_bvs_cat_axe[n]; #print(path_mesh_n)\n",
    "            #path_mesh_n = \"cup/train/cup_0039_SMPLER_centered_scaled_remeshing_iso_iter9\"\n",
    "            name_modelnet_n =  '_'.join(os.path.basename(path_mesh_n).split('_')[:3])\n",
    "            df_poids_from_modelnet.loc[n+1, 'path'] = path_mesh_n; df_poids_from_modelnet.loc[n+1, 'name'] = name_modelnet_n\n",
    "            # Load mesh PRÉALABLEMENT placé dans le REPRÈRE US (avec code Alignenment/align_mesh.ipynb)\n",
    "            path_mesh_modelnet_aligned_n = os.path.join(ModelNet40_aligned_us, path_mesh_n+\"_aligned_ok_US.obj\")\n",
    "            mesh_modelnet_aligned_n = trimesh.load_mesh(path_mesh_modelnet_aligned_n)\n",
    "            centroid_modelnet_aligned_n = get_centroid(mesh_modelnet_aligned_n.faces, mesh_modelnet_aligned_n.vertices)\n",
    "            # BVS du mesh ModelNet40 aligned\n",
    "            cams_modelnet_mesh_n, cam_bvs_modelnet_n, num_cam_bvs_modelnet_n = bvs_cams_modelnet_aligned(path_mesh_n, path_mesh_modelnet_aligned_n, dir_bvs, cams_modelnet); #print(\"Modelnet\", cam_bvs_modelnet_n, num_cam_bvs_modelnet_n)\n",
    "            \n",
    "            ## Objectif : trouver la \"BVS moyenne\"  <--> attribué un poids d'impact a chacun des cam de l'US\n",
    "            # Impact de la camera BVS de modlenet40 sur les caméras de l'étude utilisateur\n",
    "            df_poids_from_modelnet, cam_sphere = poids_modelnet_sur_US(df_poids_from_modelnet, cam_bvs_modelnet_n, centroid_modelnet_aligned_n, cams_us, R_sphere, I_us, J_us)\n",
    "\n",
    "            ## OBJ : caméras modelent40 alignées avec US\n",
    "            show_cams(mesh_modelnet_aligned_n, np.vstack((cam_bvs_modelnet_n, cam_sphere)), axe+name_modelnet_n+\"_sphere\", None, None, None, dir_outputs_visu_categorie, US_obj=False)\n",
    "\n",
    "            ###################################################################\n",
    "            ## User stuy\n",
    "            # mesh random from User_study\n",
    "            mesh_us = trimesh.load_mesh(path_mesh_us)\n",
    "            # BVS US\n",
    "            cam_bvs_us = all_bvs_us[categorie_us]['cam_bvs']\n",
    "            ## Double obj : len(cams_us) cameras US et 12 caméras modelent40 alignées avec US\n",
    "            if not(axe_sym == 'All') : show_cams(mesh_modelnet_aligned_n, cams_modelnet_mesh_n, name_modelnet_n, mesh_us, cams_us, categorie_us,  dir_outputs_visu_categorie, US_obj=True)  \n",
    "            \n",
    "            \n",
    "        ## impact de chaque mesh de la catégorie sur les caméras de l'US\n",
    "        df_poids_from_modelnet.to_csv(os.path.join(dir_outputs_csv_categorie, axe+categorie_modelnet+\"_distribution_impact-\"+str(len(X_us))+\"cams.csv\"))\n",
    "            \n",
    "        ###################################################################\n",
    "        ## Normalisation  des poids de modelnet sur US \n",
    "        # Somme des poids pour chaque caméras\n",
    "        verif = 0\n",
    "        df_poids_from_modelnet_final = pd.DataFrame(columns=['label', 'poids'])\n",
    "        for k in range(len(I_us)):\n",
    "            label_poids_k = list(df_poids_from_modelnet.loc[:, f\"{int(I_us[k])}-{J_us[k]}\"])\n",
    "            df_poids_from_modelnet_final.loc[k] = [label_poids_k[0], np.sum(label_poids_k[1:])] \n",
    "            verif += np.sum(label_poids_k[1:])\n",
    "            \n",
    "        ## verification\n",
    "        if float(abs(verif - len(paths_bvs_cat_axe))<10e-2): print(\"Somme des poids OK\")\n",
    "        else: print(\"Erreur de somme des poids\")\n",
    "\n",
    "        ###################################################################   \n",
    "        # ## BVS de la catégorie courante\n",
    "        df_poids_from_modelnet_final_sorted = df_poids_from_modelnet_final.sort_values(by='poids', ascending=False)\n",
    "        # # sauvegarde\n",
    "        df_poids_from_modelnet_final_sorted.to_csv(os.path.join(dir_outputs_csv_categorie, axe+categorie_modelnet+\"_distribution_global_modelnet-\"+str(len(X_us))+\"cams.csv\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_categorie_us = ['vase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data des bvs de US et Modelnet pour chaque catégorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicitonnaire : keys == categorie et values == info sur les BVS : cam, label, ij ....\n",
    "BVS = {}\n",
    "for axe_sym in axes_sym:\n",
    "    if axe_sym == 'All' : axe = \"\"\n",
    "    else : axe = str(axe_sym)+\"_axe_sym_\"\n",
    "    BVS_axe = {}\n",
    "    \n",
    "    for cat_us in list_categorie_us :\n",
    "        cat_m = [k for k, v in match_ModelNet2US.items() if v == cat_us][0]\n",
    "        #print(cat_m, cat_us)\n",
    "        #categorie = categorie_us; print(categorie)\n",
    "\n",
    "        ## bvs modelnet \n",
    "        df_bvs_modelnet = pd.read_csv(os.path.join(dir_outputs_csv, cat_m, axe+cat_m+\"_distribution_global_modelnet-\"+str(len(j_cam_us_ok)*8)+\"cams.csv\"))\n",
    "        label_bvs_modelnet = list(df_bvs_modelnet.loc[df_bvs_modelnet['poids'] ==  max(list(df_bvs_modelnet['poids']))]['label'])\n",
    "        position_bvs_modelenet = [[i for i in range(len(labels_us)) if labels_us[i] in label_bvs_modelnet]]\n",
    "        # coordonnées 3D des caméras BVS dans le repère de l'US\n",
    "        cam_bvs_modelnet = np.concatenate([np.array(X_us)[position_bvs_modelenet], np.array(Y_us)[position_bvs_modelenet], np.array(Z_us)[position_bvs_modelenet]]).T  \n",
    "        # Indice I-J des caméras dans le repère de l'US\n",
    "        ij_bvs_modelnet = np.concatenate([np.array(I_us)[position_bvs_modelenet], np.array(J_us)[position_bvs_modelenet]]).T\n",
    "        #print('BVS modelenet dans repère US :\\n', cam_bvs_modelnet, label_bvs_modelnet, ij_bvs_modelnet,\"\\n\")\n",
    "\n",
    "        ## bvs US\n",
    "        cam_bvs_us = all_bvs_us[['cabinet' if 'cabinet' in cat_us else cat_us][0]]['cam_bvs']\n",
    "        ij_bvs_us =  all_bvs_us[['cabinet' if 'cabinet' in cat_us else cat_us][0]]['ij_bvs']\n",
    "        #print('US : \\n', cam_bvs_us, all_bvs_us[cat_us]['label_bvs'], all_bvs_us[cat_us]['ij_bvs'])\n",
    "\n",
    "        # Sauvegarde \n",
    "        BVS_axe[cat_us] = {'modelnet': \n",
    "            {'cam': cam_bvs_modelnet, \n",
    "            'label': label_bvs_modelnet,\n",
    "            'ij': ij_bvs_modelnet, \n",
    "            'df' : df_bvs_modelnet}, \n",
    "            'US': \n",
    "            {'cam': cam_bvs_us,\n",
    "            'label': all_bvs_us[['cabinet' if 'cabinet' in cat_us else cat_us][0]]['label_bvs'], \n",
    "            'ij': all_bvs_us[['cabinet' if 'cabinet' in cat_us else cat_us][0]]['ij_bvs'], \n",
    "            'poids': all_bvs_us[['cabinet' if 'cabinet' in cat_us else cat_us][0]]['poids_bvs'], \n",
    "            'df' : all_bvs_us[['cabinet' if 'cabinet' in cat_us else cat_us][0]]['df']}, 'metriques': {}, 'metriques_sym': {}}\n",
    "        \n",
    "    # Sauvegarde en fonction du nb de sym\n",
    "    BVS[axe_sym] = BVS_axe\n",
    "print(BVS.keys(),\"\\n\",BVS['All'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PS est le meme avec ou sans symetrique car on les prends déjà en compte dans la formule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce que je prédis :\n",
    "print(get_ds2(coord_U= [1.10,1.56,1.1], coord_M=[0, 1.56, 1.56], sig=1.5, epsilon=2), 19.4/34.1)\n",
    "\n",
    "for axe_sym in axes_sym:\n",
    "    if axe_sym == 'All' : axe = \"\"\n",
    "    else : axe = str(axe_sym)+\"_axe_sym_\"\n",
    "    print(\"\\n\",axe_sym)\n",
    "    for cat_us in list_categorie_us:\n",
    "        print(\"\\n\",cat_us)\n",
    "        # Proximity score : Magenta : US // Sym uS : Bleu // Modelnet : Vert\n",
    "        PS, terme = score_proximite(axe_sym, BVS[axe_sym], cat_us, path_mesh_us, list_categroie_us_sym, data_us_cam, dir_outputs_visu_categorie, sig =1.5, epsilon=2); print(PS)\n",
    "        BVS[axe_sym][cat_us]['metriques']['PS'] = (np.round(PS,3), terme)\n",
    "        BVS[axe_sym][cat_us]['metriques_sym']['PS'] = (np.round(PS,3), terme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour normaliser les histogrammes (avec ou sans symétriques), on divise chacun par sa somme, comme ça la somme vaut 1 et les histogrammes peuvent être assimilés à des densité de probabilité discretes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dossier pour les visualisations des histogrammes \n",
    "dir_outputs_visu_histo = os.path.join(dir_outputs_visu, 'histogrammes'); print(dir_outputs_visu_histo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec symétriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exemple, _ = get_poids_from_BVS(BVS['All'], cat_us, labels_us, data_us_cam); df_exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metriques = {\"Dnom\" : lambda w1, w2 : Dnom(w1, w2),\n",
    "                  \"Dord\" :  lambda w1, w2 : Dord(w1, w2),\n",
    "                  \"Dmod\" :  lambda w1, w2 : Dmod(w1, w2),\n",
    "                  \"L2_norm\" : lambda w1, w2 : np.linalg.norm(w1 - w2),\n",
    "                  \"L1_norm\" : lambda w1, w2 : np.sum(np.abs(w1 - w2)),\n",
    "                  \"cos_sim\" : lambda w1, w2 : 1 - cosine(w1, w2),\n",
    "                  \"fourier_dist\" : lambda w1, w2 : fourier_similarity(w1, w2),\n",
    "                  \"corr-pv\" : lambda w1, w2 : pearsonr(w1, w2),\n",
    "                  \"kl_div\" : lambda w1, w2 : entropy(w2, w1),\n",
    "                  \"js_div\" : lambda w1, w2 : jensenshannon(w1, w2),\n",
    "                  \"bhatta_dist\" : lambda w1, w2 : -np.log(np.sum(np.sqrt(np.array(w1) * np.array(w2)))), \n",
    "                  \"circu_dist\" : lambda w1, w2 : np.sum(np.minimum(np.abs(w1 - w2), 1 - np.abs(w1 - w2)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for axe_sym in tqdm.tqdm(axes_sym):\n",
    "    if axe_sym == 'All' : axe = \"\"\n",
    "    else : axe = str(axe_sym)+\"_axe_sym_\"\n",
    "    \n",
    "    for cat_us in list_categorie_us:\n",
    "        # dossier pour les visualisations\n",
    "        cat_m = [k for k, v in match_ModelNet2US.items() if v == cat_us][0]\n",
    "        dir_outputs_visu_histo_cat = os.path.join(dir_outputs_visu_histo, cat_m)\n",
    "        \n",
    "        df_poids_both, _ = get_poids_from_BVS(BVS[axe_sym], cat_us, labels_us, data_us_cam)\n",
    "        # sauvegarde poids \n",
    "        df_poids_both.to_csv(os.path.join(dir_outputs_csv, cat_m, axe+cat_us+\"_poids_both.csv\"))\n",
    "        # Poids des caméras pour les deux méthodes (exemple)\n",
    "        weights_method_us = np.array(df_poids_both['poids_us_norm'])\n",
    "        weights_method_m = np.array(df_poids_both['poids_modelnet_norm'])\n",
    "        ####### Visualisation des poids \n",
    "        cams = [list(data_us_cam.loc[data_us_cam['label'] == l].iloc[0])[6:9] for l in df_poids_both['label']]\n",
    "        # pour avoir de jolies couleurs il faut que les poids soient entre 0 et 1\n",
    "        weights_histo_us = [x / max(weights_method_us) for x in weights_method_us]\n",
    "        show_cams_histogram(axe+cat_m+\"_histograms_us\", cams, weights_histo_us, dir_outputs_visu_histo_cat) \n",
    "        \n",
    "        cams_ecartees = [[3 * x / 2.2, y, 3*z/2.2] for x,y,z in cams ]\n",
    "        weights_histo_m = [x / max(weights_method_m) for x in weights_method_m]\n",
    "        show_cams_histogram(axe+cat_m+\"_histograms_m\", cams_ecartees, weights_histo_m, dir_outputs_visu_histo_cat)\n",
    "        \n",
    "        # En 2D\n",
    "        histogram_circulaire(weights_method_us, weights_method_m, dir_outputs_visu_histo_cat, axe+cat_us)\n",
    "        histograms_2D(weights_method_us, weights_method_m, np.array(df_poids_both['label']), dir_outputs_visu_histo_cat, axe+cat_us)\n",
    "        ####### Métriques\n",
    "        for name, metrique in dict_metriques.items():\n",
    "            BVS[axe_sym][cat_us]['metriques'][name] = np.round(metrique(weights_method_us, weights_method_m),3)\n",
    "        \n",
    "    ## Valeur idéales \n",
    "    ideal ={}; a = weights_method_us; ideal['PS'] = 1\n",
    "    for name, metrique in dict_metriques.items():\n",
    "        ideal[name] = np.round(metrique(a, a),3)\n",
    "    \n",
    "    # Metriques\n",
    "    metriques = list(BVS[axe_sym][cat_us]['metriques'].keys())\n",
    "    df_metriques = pd.DataFrame(columns=['categorie - '+axe_sym+ ' axe sym'] + metriques)\n",
    "    df_metriques.loc[len(df_metriques)] = ['ideal']+[ideal[m] for m in metriques]\n",
    "    for cat_us in list_categorie_us:\n",
    "        cat_m = [k for k, v in match_ModelNet2US.items() if v == cat_us][0]\n",
    "        ## nb bvs dispo en fonction des axes de symétrie\n",
    "        if axe_sym == 'All' : nb_bvs_axe_sym = dict_bvs_dispo[cat_m]\n",
    "        else : nb_bvs_axe_sym = len(dict_bvs_dispo_axe_sym[cat_m][axe_sym])\n",
    "        df_metriques.loc[len(df_metriques)+1] = [cat_us+\" - \"+str(nb_bvs_axe_sym)+\"/\"+str(dict_bvs_dispo[cat_m])]+[BVS[axe_sym][cat_us]['metriques'][m] for m in metriques]\n",
    "        \n",
    "    df_metriques.to_csv(os.path.join(dir_outputs_csv, axe+\"metriques_modelnet40_user_study.csv\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sans Symétrique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les symétriques, on somme les pids, car chaque précédement on a normalier pour que chaque objet ait un impact de 1, donc en tout on a que la somme de tous les poids == nb objet. Si on prend le poids max entre les symétriques, on perd cette égalité. Donc on prend la somme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, df_exemple_sym = get_poids_from_BVS(BVS['All'], cat_us, labels_us, data_us_cam); df_exemple_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for axe_sym in tqdm.tqdm(axes_sym):\n",
    "    if axe_sym == 'All' : axe = \"\"\n",
    "    else : axe = str(axe_sym)+\"_axe_sym_\"\n",
    "    for cat_us in list_categroie_us_sym:\n",
    "        cat_m = [k for k, v in match_ModelNet2US.items() if v == cat_us][0]\n",
    "        dir_outputs_visu_histo_cat = os.path.join(dir_outputs_visu_histo, cat_m)\n",
    "        \n",
    "        _, df_poids_both_sym = get_poids_from_BVS(BVS[axe_sym], cat_us, labels_us, data_us_cam)\n",
    "        # sauvegarde poids \n",
    "        df_poids_both_sym.to_csv(os.path.join(dir_outputs_csv, cat_m, axe+cat_us+\"_poids_both_sym.csv\"))\n",
    "        # Poids des caméras pour les deux méthodes (exemple)\n",
    "        weights_method_us = np.array(df_poids_both_sym['poids_us_norm'])\n",
    "        weights_method_m = np.array(df_poids_both_sym['poids_modelnet_norm'])\n",
    "        ####### Visualisation des poids \n",
    "        cams = [list(data_us_cam.loc[data_us_cam['label'] == l].iloc[0])[6:9] for l in df_poids_both_sym['label']]\n",
    "        show_cams_histogram(axe+cat_m+\"_histograms_us_sym\", cams, weights_method_us, dir_outputs_visu_histo_cat) \n",
    "        \n",
    "        cams_ecartees = [[3 * x / 2.2, y, 3*z/2.2] for x,y,z in cams ]\n",
    "        show_cams_histogram(axe+cat_m+\"_histograms_m_sym\", cams_ecartees, weights_method_m, dir_outputs_visu_histo_cat)\n",
    "        \n",
    "        # En 2D\n",
    "        histogram_circulaire(weights_method_us, weights_method_m, dir_outputs_visu_histo_cat, axe+cat_us+\"_sym_\")\n",
    "        histograms_2D(weights_method_us, weights_method_m, np.array(df_poids_both_sym['label']), dir_outputs_visu_histo_cat, axe+cat_us+\"_sym_\")\n",
    "        \n",
    "        ####### Métriques\n",
    "        for name, metrique in dict_metriques.items():\n",
    "            BVS[axe_sym][cat_us]['metriques_sym'][name] = np.round(metrique(weights_method_us, weights_method_m),3)\n",
    "        \n",
    "    ## Valeur idéales \n",
    "    ideal ={}; a = weights_method_us; ideal['PS'] = 1\n",
    "    for name, metrique in dict_metriques.items():\n",
    "        ideal[name] = np.round(metrique(a, a),3)\n",
    "\n",
    "    # Metriques\n",
    "    metriques_sym = list(BVS[axe_sym][cat_us]['metriques_sym'].keys())\n",
    "    df_metriques_sym = pd.DataFrame(columns=['categorie - '+axe_sym+ ' axe sym']  + metriques_sym)\n",
    "    df_metriques_sym.loc[len(df_metriques_sym)] = ['ideal']+[ideal[m] for m in metriques]\n",
    "    for cat_us in list_categroie_us_sym:\n",
    "        #df_metriques_sym.loc[len(df_metriques_sym)+1] = [cat_us+\"_sym\"]+[BVS[axe_sym][cat_us]['metriques_sym'][m] for m in metriques_sym]\n",
    "        cat_m = [k for k, v in match_ModelNet2US.items() if v == cat_us][0]\n",
    "        ## nb bvs dispo en fonction des axes de symétrie\n",
    "        if axe_sym == 'All' : nb_bvs_axe_sym = dict_bvs_dispo[cat_m]\n",
    "        else : nb_bvs_axe_sym = len(dict_bvs_dispo_axe_sym[cat_m][axe_sym])\n",
    "        df_metriques_sym.loc[len(df_metriques_sym)+1] = [cat_us+\"_sym\"+\" - \"+str(nb_bvs_axe_sym)+\"/\"+str(dict_bvs_dispo[cat_m])]+[BVS[axe_sym][cat_us]['metriques_sym'][m] for m in metriques_sym]\n",
    "        \n",
    "    df_metriques_sym.to_csv(os.path.join(dir_outputs_csv, axe+\"metriques_modelnet40_user_study_sym.csv\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation globales des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score du terme C dans PS en fonction de leccart entre les caméras\n",
    "cam_u = [1.56,1.56,0]; sss=1.5\n",
    "print(\"egalite\", get_ds2(coord_U= cam_u, coord_M=cam_u, sig=sss, epsilon=2),\"-- 1 pas : \", get_ds2(coord_U= cam_u, coord_M=[1.1,1.56,1.1], sig=sss, epsilon=2), \"-- 2 pas : \", get_ds2(coord_U= cam_u, coord_M=[0,1.56,1.56], sig=sss, epsilon=2), \"-- 3 pas : \", get_ds2(coord_U= cam_u, coord_M=[-1.1,1.56,1.1], sig=sss, epsilon=2), \"-- 4 pas : \", get_ds2(coord_U= cam_u, coord_M=[-1.56,1.56,0], sig=sss, epsilon=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metriques_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comparaison_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
